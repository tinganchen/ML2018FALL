#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ML hw2_1 Generative Probabislistic Model
"""

import numpy as np
import pandas as pd
#import sys

"""
1. Import files
"""
raw_x_train = pd.read_csv('data/train_x.csv', encoding = 'big5') # sys.argv[1]
y_train = pd.read_csv('data/train_y.csv' , encoding = 'big5') #sys.argv[2]


"""
2. Validation Split
"""
def split_into_tr_val(x_train, y_train, fold = 5):
    val1_idx = np.array([i for i in range(x_train.shape[0]) if i % fold == 0])
    x_val = []
    y_val = []
    x_tr = []
    y_tr = []
    for i in range(fold):
        val_idx = val1_idx + i
        tr_idx = np.array(list(set(range(x_train.shape[0]))-set(val_idx)))
        
        x_val.append(x_train.iloc[val_idx, :])
        y_val.append(y_train.iloc[val_idx, :])
        x_tr.append(x_train.iloc[tr_idx, :])
        y_tr.append(y_train.iloc[tr_idx, :])
    return x_val, y_val, x_tr, y_tr

"""
3_2. Modeling
@ Generative Probabilistic Model
"""
class Generative_Prob_Model():
    def __init__(self, data_x, data_y, categ_ft_list, num_ft_list):
        self.categ_ft_list = categ_ft_list
        self.num_ft_list = num_ft_list
        self.n_x = data_x[self.num_ft_list]
        self.c_x = data_x[self.categ_ft_list]
        self.y = data_y
    
    # Numeric features - posterior prob. generated by multivariate normal distr.
    def prior_prob_mean_cov(self):
        counts_per_class = np.unique(self.y, return_counts = True)[1]
        n = len(self.y)
        self.prob_per_class = counts_per_class / n
        self.n_class = len(self.prob_per_class)
        data = pd.concat([self.n_x, self.y], 1)
        # means for classes
        mean_matrices = np.array(data.groupby(self.y.columns.values[0]).mean())
        cov_pd = pd.DataFrame(np.array(data)).groupby(self.n_x.shape[1]).cov()
        # covariance matrices for classes
        cov_matrices = [cov_pd.iloc[i*cov_pd.shape[1]:(i+1)*cov_pd.shape[1], :] for i in range(self.n_class)]
        # weighted matrix
        cov_matrices_w = [cov_matrices[i] * self.prob_per_class[i] for i in range(self.n_class)]
        sum_pd = pd.DataFrame(np.zeros([self.n_x.shape[1], self.n_x.shape[1]]))
        for j in cov_matrices_w :
            sum_pd += pd.DataFrame(np.array(j))
        # pooled covariance matrix
        pool_cov_mat =  sum_pd
        return mean_matrices, pool_cov_mat
    
    def multivariate_normal(self, x_row, y_row):
        means, cov = self.prior_prob_mean_cov()
        mean = means[y_row] 
        d = len(mean)          
        pdf = 1 / np.power(2 * np.pi, d/2) / np.power(np.linalg.det(cov), 1/2) * np.exp(-1/2 * np.dot(np.dot((x_row - mean).T, np.linalg.inv(cov)), x_row - mean))
        return pdf   
    
    # Categorical features - prob. calculated by bayesian's rule
    def bayes(self):
        data = pd.concat([self.c_x, self.y], 1)
        p_ft_class = []
        keys = []
        for clss in range(len(np.unique(self.y))):
            p_ft = []
            key = []
            for ft in range(self.c_x.shape[1]):
                data_clss = data[data.iloc[:, -1] == clss]
                proportion_class = np.unique(data_clss.iloc[:, ft], return_counts = True)[1] / len(data_clss)
                key.append(np.unique(data_clss.iloc[:, ft], return_counts = True)[0])
                p_ft.append(proportion_class)
            p_ft_class.append(p_ft)
            keys.append(key)
        return p_ft_class, keys
    
    def prior_prob_categ(self, x, class_belong):
        p_ft_class, keys = self.bayes()
        posterior_prob = []
        for i in range(len(x)):
            prob = []
            for clss in np.unique(self.y):
                ps_given_class = p_ft_class[np.int(clss)]
                x_row = x.iloc[i, :]
                p_given_class = []
                
                for j in range(len(x_row)):
                    if x_row[j] in list(keys[clss][j]):
                        idx = list(keys[clss][j]).index(x_row[j])
                        p = ps_given_class[j][idx]
                        p_given_class.append(p * self.prob_per_class[clss])
                    else:
                        p_given_class.append(1e-20)
                p_given_class_prod = np.prod(p_given_class)
                prob.append(p_given_class_prod)
            posterior_prob.append(prob)
        return posterior_prob
            
    def pred(self, x, class_belong):
        means, cov = self.prior_prob_mean_cov()
        posterior_prob = self.prior_prob_categ(x[self.categ_ft_list], class_belong)
        x = pd.DataFrame(np.array(x[self.num_ft_list]))
        posterior_probs = []
        for i in range(len(x)):
            x_row = x.iloc[i, :]
            prior_probs = [self.multivariate_normal(x_row, clss) * self.prob_per_class[clss]  for clss in np.unique(self.y)]
            post_prob = prior_probs[class_belong] * posterior_prob[i][class_belong] / np.sum(np.array(prior_probs) * posterior_prob[i][class_belong])
            posterior_probs.append(post_prob)
        return posterior_probs

    def compute_accuracy(self, x, class_belong, y):
        y_hat = self.pred(x, class_belong)
        return np.mean(np.round(y_hat) == np.array(y))


def save_csv(pred, filename):
    col = []
    for i in range(pred.shape[0]):
        col.append('id_{}'.format(i))       
    with open(filename, 'w', newline = '\n') as f:
        f.writelines('id,value\n')
        for i in range(pred.shape[0]):
            f.writelines(','.join([col[i], str(pred[i])+'\n']))

"""
4. Data Preprocessing
"""
"""
# 1. Concatenate x_train and y_train
raw_x = pd.concat([raw_x_train, raw_x_test], axis = 0)
raw_x.index = np.arange(len(raw_x))
x = pd.DataFrame(np.copy(raw_x))
x.columns = raw_x.columns

features_name = raw_x_train.columns.values
categorical_features = features_name[np.delete(np.arange(1, 11), 3)]
numeric_features = features_name[list(set(np.arange(len(features_name))) - 
                                      set(np.delete(np.arange(1, 11), 3)))]

# 2. Categorical features - re-categorize
# (a) Sex (1 = male; 2 = female)
# (b) Education (0 = others; 1 = graduate school; 2 = university; 3 = high school)
x[categorical_features[1]] = raw_x[categorical_features[1]].replace([4, 5, 6], 0)
# (c) Marriage status (1 = married; 2 = single; 3 = others)
x[categorical_features[2]] = raw_x[categorical_features[2]].replace(0, 3)
# (d) PAY_0 ~ PAY_6
#  0 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months;... 
#  9 = payment delay for nine months.
for i in range(3, len(categorical_features)):
    x[categorical_features[i]] = raw_x[categorical_features[i]].replace([-2, -1], 0)
x_x_train = x.iloc[:20000, :]
x_x_test = x.iloc[20000:, :]
x_x_test.index = raw_x_test.index

# 3. Numeric featres - outliers processing
def replace_outliers_tr(df, colnames):
    df2 = pd.DataFrame(np.copy(df))
    df2.columns = df.columns
    q1s = []
    q3s = []
    for i in range(len(colnames)):
        col = np.array(df[colnames[i]])
        q1 = np.percentile(col, 25)
        q3 = np.percentile(col, 75)
        iqr = q3 - q1
        idx2small = np.where(col < q1 - 1.5 * iqr)
        idx2large = np.where(col > q3 + 1.5 * iqr)
        col[idx2small] = q1 - 1.5 * iqr
        col[idx2large] = q3 + 1.5 * iqr
        df2[colnames[i]] = col
        q1s.append(q1)
        q3s.append(q3)
    return df2, q1s, q3s

x_train, x_train_q1, x_train_q3 = replace_outliers_tr(x_x_train, numeric_features)

def replace_outliers_te(df, colnames, q1s_tr, q3s_tr):
    df2 = pd.DataFrame(np.copy(df))
    df2.columns = df.columns
    for i in range(len(colnames)):
        col = np.array(df[colnames[i]])
        q1 = q1s_tr[i]
        q3 = q3s_tr[i]
        iqr = q3 - q1
        idx2small = np.where(col < q1 - 1.5 * iqr)
        idx2large = np.where(col > q3 + 1.5 * iqr)
        col[idx2small] = q1 - 1.5 * iqr
        col[idx2large] = q3 + 1.5 * iqr
        df2[colnames[i]] = col
    return df2

x_test = replace_outliers_te(x_x_test, numeric_features, x_train_q1, x_train_q3)

# 3. Numeric featres 
# 3_1. Scaling
moments = x_train[numeric_features].mean(0), x_train[numeric_features].std(0)
x_train_scale = pd.DataFrame(np.copy(x_train))
x_train_scale.columns = x_train.columns
x_train_scale[numeric_features] = (x_train[numeric_features] - moments[0]) / moments[1]

x_test_scale = pd.DataFrame(np.copy(x_test))
x_test_scale.columns = x_test.columns
x_test_scale[numeric_features] = (x_test[numeric_features] - moments[0]) / moments[1]

x_val, y_val, x_tr, y_tr = split_into_tr_val(x_train_scale, y_train, 5)

np.save('gpm_tr_preprocess.npy', (x_tr, y_tr))
np.save('gpm_val_preprocess.npy', (x_val, y_val))
"""

features_name = raw_x_train.columns.values
categorical_features = features_name[np.delete(np.arange(1, 11), 3)]
numeric_features = features_name[list(set(np.arange(len(features_name))) - 
                                      set(np.delete(np.arange(1, 11), 3)))]
x_val, y_val = np.load('gpm_val_preprocess.npy')
x_tr, y_tr = np.load('gpm_tr_preprocess.npy')

"""
5. Training + Testing

gpm_interact_pred_prob = gpm_interact.pred(x_test_scale, 1)
gpm_interact_pred = np.array([np.int(i) for i in np.round(gpm_interact_pred_prob)])
np.save('gpm_interact.npy', gpm_interact_pred)
"""
gpm_interact = Generative_Prob_Model(x_tr[4], y_tr[4], categorical_features, numeric_features)
#val_acc = gpm_interact.compute_accuracy(x_val[4], 1, y_val[4])
#val_acc
# set0: 0.784107875
# set1: 0.76575
# set2: 0.7805
# set3: 0.76975
# set4: 0.7885
# gpm_interact.compute_accuracy(x_train_scale, 1, y_train) # 0.777611125
#